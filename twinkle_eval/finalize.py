import glob
import json
import os
from collections import defaultdict
from typing import Optional

import numpy as np

from .results_exporters import ResultsExporterFactory


def finalize_results(timestamp: str, hf_repo_id: Optional[str] = None, hf_variant: Optional[str] = "default") -> int:
    """åˆä½µå¹³è¡Œé‹ç®—ç”¢ç”Ÿçš„ç¢ç‰‡ä¸¦é‡æ–°è¨ˆç®—è©•æ¸¬æŒ‡æ¨™ï¼Œæœ€å¾Œåˆªé™¤ç¢ç‰‡ã€‚
    è‹¥ç„¡ç¢ç‰‡ä½†å­˜åœ¨å–®ç¯€é»æœ€çµ‚çµæœï¼Œå‰‡ç›´æ¥åŸ·è¡Œä¸Šå‚³ã€‚
    """

    results_dir = "results"
    json_shards = sorted(glob.glob(os.path.join(results_dir, f"results_{timestamp}_node*_rank*.json")))

    if not json_shards:
        # å–®ç¯€é»åŸ·è¡Œï¼šç›´æ¥ä¸Šå‚³å·²å­˜åœ¨çš„æœ€çµ‚çµæœï¼Œç„¡é ˆåˆä½µ
        single_node_result = os.path.join(results_dir, f"results_{timestamp}.json")
        if not os.path.exists(single_node_result):
            print(f"æ‰¾ä¸åˆ°æ™‚é–“æˆ³è¨˜ç‚º {timestamp} çš„è©•æ¸¬ç¢ç‰‡æˆ–æœ€çµ‚çµæœã€‚")
            return 1

        print(f"æœªç™¼ç¾åˆ†æ•£å¼ç¢ç‰‡ï¼Œä»¥å–®ç¯€é»æ¨¡å¼ç›´æ¥ä¸Šå‚³ {single_node_result}ã€‚")
        if hf_repo_id:
            try:
                from .hf_uploader import upload_results
                with open(single_node_result, "r", encoding="utf-8") as _f:
                    _result = json.load(_f)
                model_name = _result.get("config", {}).get("model", {}).get("name", "unknown_model")
                upload_results(
                    repo_id=hf_repo_id,
                    variant=hf_variant,
                    model_name=model_name,
                    results_dir=results_dir,
                    timestamp=timestamp,
                )
                print("ä¸Šå‚³å®Œæˆã€‚")
            except Exception as e:
                print(f"ä¸Šå‚³è‡³ Hugging Face å¤±æ•—: {e}")
                return 1
        return 0

    print(f"æ‰¾åˆ° {len(json_shards)} å€‹ JSON é…ç½®ç¢ç‰‡ã€‚é–‹å§‹åˆä½µèˆ‡çµ±æ•´æ•¸æ“š...")

    with open(json_shards[0], "r", encoding="utf-8") as f:
        base_result = json.load(f)

    # â”€â”€ ç¬¬ä¸€éšæ®µï¼šæ”¶é›†å…ƒè³‡æ–™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # run_idx -> å±¬æ–¼è©² run çš„æ‰€æœ‰åˆ†ç‰‡ JSONL è·¯å¾‘åˆ—è¡¨
    run_jsonl_shards: dict[int, list[str]] = defaultdict(list)
    # ds_name -> file_path -> è©²æª”æ¡ˆåŒ…å«çš„ run_idx é›†åˆ
    ds_file_runs: dict[str, dict[str, set[int]]] = defaultdict(lambda: defaultdict(set))

    for shard_path in json_shards:
        with open(shard_path, "r", encoding="utf-8") as f:
            shard_data = json.load(f)

        for ds_name, ds_data in shard_data.get("dataset_results", {}).items():
            for file_res in ds_data.get("results", []):
                file_path = file_res["file"]
                jsonl_paths = file_res.get("individual_runs", {}).get("results", [])
                for run_idx, jsonl_path in enumerate(jsonl_paths):
                    ds_file_runs[ds_name][file_path].add(run_idx)
                    if jsonl_path not in run_jsonl_shards[run_idx]:
                        run_jsonl_shards[run_idx].append(jsonl_path)

    # â”€â”€ ç¬¬äºŒéšæ®µï¼šä¸²æµåˆä½µæ‰€æœ‰åˆ†ç‰‡ JSONL â†’ æ¯å€‹ run ç”¢ç”Ÿä¸€å€‹åˆä½µå¾Œçš„ JSONL â”€
    # run_idx -> åˆä½µå¾Œçš„ JSONL è·¯å¾‘
    run_merged_path: dict[int, str] = {}
    merged_jsonl_files: list[str] = []
    shard_jsonl_files: set[str] = set()  # å¾…æ¸…ç†çš„åŸå§‹åˆ†ç‰‡ JSONL

    try:
        for run_idx, shard_paths in sorted(run_jsonl_shards.items()):
            merged_name = os.path.join(results_dir, f"eval_results_{timestamp}_run{run_idx}.jsonl")
            run_merged_path[run_idx] = merged_name
            merged_jsonl_files.append(merged_name)

            # å¾æ­¤ run çš„æ¯å€‹åˆ†ç‰‡è’é›†æ‰€æœ‰è¨˜éŒ„
            all_entries: list[dict] = []
            for j_path in shard_paths:
                if os.path.exists(j_path):
                    shard_jsonl_files.add(j_path)
                    with open(j_path, "r", encoding="utf-8") as jf:
                        for line in jf:
                            line = line.strip()
                            if line:
                                all_entries.append(json.loads(line))

            # ä¾ question_id æ’åºå¾Œä¸€æ¬¡å¯«å…¥ï¼Œé¿å…åè¦†é–‹æª”
            all_entries.sort(key=lambda x: int(x.get("question_id", 0)))
            print(f"  run{run_idx}: åˆä½µ {len(shard_paths)} å€‹ç¢ç‰‡ â†’ {len(all_entries)} ç­†è¨˜éŒ„")

            with open(merged_name, "w", encoding="utf-8") as jf:
                for entry in all_entries:
                    jf.write(json.dumps(entry, ensure_ascii=False) + "\n")

        # â”€â”€ ç¬¬ä¸‰éšæ®µï¼šæƒæåˆä½µå¾Œçš„ JSONLï¼Œä¾ä¾†æºæª”æ¡ˆçµ±è¨ˆæ­£ç¢ºç‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # run_idx -> file_path -> æ˜¯å¦æ­£ç¢ºçš„å¸ƒæ—ä¸²åˆ—
        run_file_correct: dict[int, dict[str, list]] = defaultdict(lambda: defaultdict(list))

        for run_idx, merged_path in run_merged_path.items():
            if not os.path.exists(merged_path):
                continue
            with open(merged_path, "r", encoding="utf-8") as jf:
                for line in jf:
                    line = line.strip()
                    if not line:
                        continue
                    entry = json.loads(line)
                    # å„ªå…ˆä½¿ç”¨ source_fileï¼Œå…¶æ¬¡å˜—è©¦ file æ¬„ä½
                    src_file = entry.get("source_file") or entry.get("file")
                    if src_file:
                        run_file_correct[run_idx][src_file].append(entry.get("is_correct", False))

        # â”€â”€ ç¬¬å››éšæ®µï¼šçµ„è£æœ€çµ‚çµæœçµæ§‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        merged_dataset_results: dict = {}

        for ds_name, files_map in ds_file_runs.items():
            ds_results = []
            for file_path, run_indices in files_map.items():
                run_accuracies = []
                run_merged_jsonl_paths = []

                for run_idx in sorted(run_indices):
                    merged_path = run_merged_path.get(run_idx)
                    if merged_path:
                        run_merged_jsonl_paths.append(merged_path)

                    # å„ªå…ˆå¾ç¬¬ä¸‰éšæ®µçš„çµ±è¨ˆçµæœå–å¾—æ­£ç¢ºç‡
                    correct_list = run_file_correct.get(run_idx, {}).get(file_path)
                    if correct_list is not None:
                        acc = sum(correct_list) / len(correct_list) if correct_list else 0.0
                    else:
                        # å‚™æ´ï¼šç›´æ¥å¾åŸå§‹åˆ†ç‰‡ JSON è®€å–é å…ˆè¨ˆç®—çš„æ­£ç¢ºç‡
                        acc = _acc_from_shards(json_shards, ds_name, file_path, run_idx)
                    run_accuracies.append(acc)

                mean_acc = float(np.mean(run_accuracies)) if run_accuracies else 0.0
                std_acc = float(np.std(run_accuracies)) if len(run_accuracies) > 1 else 0.0

                ds_results.append({
                    "file": file_path,
                    "accuracy_mean": mean_acc,
                    "accuracy_std": std_acc,
                    "individual_runs": {
                        "accuracies": run_accuracies,
                        "results": run_merged_jsonl_paths,
                    },
                })

            ds_avg_acc = float(np.mean([r["accuracy_mean"] for r in ds_results])) if ds_results else 0.0
            ds_avg_std = float(np.mean([r["accuracy_std"] for r in ds_results])) if ds_results else 0.0

            merged_dataset_results[ds_name] = {
                "results": ds_results,
                "average_accuracy": ds_avg_acc,
                "average_std": ds_avg_std,
            }

        final_results = {
            "timestamp": timestamp,
            "config": base_result["config"],
            "duration_seconds": base_result.get("duration_seconds", 0),
            "dataset_results": merged_dataset_results,
        }

        base_output_path = os.path.join(results_dir, f"results_{timestamp}")
        exported_files = ResultsExporterFactory.export_results(
            final_results, base_output_path, ["json"], base_result["config"]
        )
        print(f"âœ… åˆä½µå®Œæˆï¼Œçµæœå·²åŒ¯å‡ºè‡³: {', '.join(exported_files)}")

    finally:
        # â”€â”€ æ¸…ç†ï¼šå³ä½¿ç¨‹å¼ä¸­é€”è¢«ä¸­æ–·æˆ–è¨˜æ†¶é«”ä¸è¶³ä¹Ÿä¿è­‰åŸ·è¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("ğŸ§¹ æ¸…ç† Rank åˆ†æ•£å¼ç¢ç‰‡...")
        for sp in json_shards:
            try:
                os.remove(sp)
            except OSError:
                pass
        for jp in shard_jsonl_files:
            try:
                os.remove(jp)
            except OSError:
                pass

    if hf_repo_id:
        try:
            from .hf_uploader import upload_results
            model_name = base_result["config"].get("model", {}).get("name", "unknown_model")
            upload_results(
                repo_id=hf_repo_id,
                variant=hf_variant,
                model_name=model_name,
                results_dir=results_dir,
                timestamp=timestamp,
            )
            print("âœ… æˆåŠŸä¸Šå‚³åˆä½µçµæœè‡³ Hugging Face")
        except Exception as e:
            print(f"âŒ ä¸Šå‚³è‡³ Hugging Face å¤±æ•—: {e}")

    return 0


def _acc_from_shards(json_shards: list[str], ds_name: str, file_path: str, run_idx: int) -> float:
    """å‚™æ´å‡½å¼ï¼šç›´æ¥å¾å„åˆ†ç‰‡ JSON è®€å–å·²é å…ˆè¨ˆç®—çš„æ­£ç¢ºç‡ï¼ˆç•¶ JSONL å…§ç„¡ä¾†æºæª”æ¡ˆæ¬„ä½æ™‚ä½¿ç”¨ï¼‰"""
    accs = []
    for sp in json_shards:
        with open(sp, "r", encoding="utf-8") as f:
            shard = json.load(f)
        for fr in shard.get("dataset_results", {}).get(ds_name, {}).get("results", []):
            if fr["file"] == file_path:
                run_accs = fr.get("individual_runs", {}).get("accuracies", [])
                if run_idx < len(run_accs):
                    accs.append(run_accs[run_idx])
    return float(np.mean(accs)) if accs else 0.0


# å‘å¾Œç›¸å®¹åˆ¥å
merge_distributed_results = finalize_results
